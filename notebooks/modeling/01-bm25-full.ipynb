{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 03:18:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from longeval.spark import get_spark\n",
    "from longeval.collection import ParquetCollection\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pathlib import Path\n",
    "from opensearchpy import OpenSearch\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "spark = get_spark()\n",
    "root = Path(\"~/\").expanduser() / \"scratch/longeval\"\n",
    "collection = ParquetCollection(spark, f\"{root}/parquet/train/2023_01/English\")\n",
    "\n",
    "relevant_queries = collection.queries.join(\n",
    "    collection.qrels.where(\"rel > 0\")\n",
    "    .groupBy(\"qid\")\n",
    "    .agg(F.collect_set(\"docid\").alias(\"rel_docids\")),\n",
    "    on=\"qid\",\n",
    ").select(\"qid\", \"query\", \"rel_docids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|  size(rel_docids)|\n",
      "+-------+------------------+\n",
      "|  count|               599|\n",
      "|   mean| 7.282136894824708|\n",
      "| stddev|2.0460087848745068|\n",
      "|    min|                 6|\n",
      "|    max|                19|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevant_queries.select(F.size(\"rel_docids\")).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bulk_query(df, index_name: str, k: int = 50) -> list[dict]:\n",
    "    data = []\n",
    "    for row in df.itertuples():\n",
    "        data += [\n",
    "            {\n",
    "                \"index\": index_name,\n",
    "            },\n",
    "            {\n",
    "                \"query\": {\n",
    "                    \"match\": {\n",
    "                        \"contents\": {\n",
    "                            \"query\": row.query,\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"_source\": False,\n",
    "                \"size\": k,\n",
    "            },\n",
    "        ]\n",
    "    return data\n",
    "\n",
    "\n",
    "@F.udf(\"struct<ndcg: double, rel_count: int>\")\n",
    "def compute_scores(\n",
    "    docids: list[str], scores: list[float], rel_docids: list[str]\n",
    ") -> dict:\n",
    "    if not docids:\n",
    "        return {\"ndcg\": 0.0, \"rel_count\": 0}\n",
    "    true_rel = [1 if docid in rel_docids else 0 for docid in docids]\n",
    "    pred_rel = scores\n",
    "\n",
    "    # handle the case where there is only a single document, which the sklearn\n",
    "    # metric does not handle well\n",
    "    if len(true_rel) == 1:\n",
    "        score = true_rel[0]\n",
    "        return {\"ndcg\": float(score), \"rel_count\": int(score)}\n",
    "    return {\n",
    "        \"ndcg\": float(ndcg_score([true_rel], [pred_rel])),\n",
    "        \"rel_count\": sum(true_rel),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_search(relevant_queries, index_name: str, host=\"localhost:9200\") -> list[dict]:\n",
    "    client = OpenSearch(host)\n",
    "\n",
    "    pdf = relevant_queries.toPandas()\n",
    "    results = client.msearch(generate_bulk_query(pdf, index_name))\n",
    "    # now iterate over the results and add in the original query\n",
    "    for row, obj in zip(pdf.itertuples(), results[\"responses\"]):\n",
    "        obj[\"qid\"] = row.qid\n",
    "\n",
    "    schema = \"\"\"\n",
    "        qid: string,\n",
    "        hits: struct<\n",
    "            total: struct<value: long, relation: string>,\n",
    "            max_score: double,\n",
    "            hits: array<struct<_index: string, _id: string, _score: double>>\n",
    "        >\n",
    "    \"\"\"\n",
    "    resp = (\n",
    "        spark.createDataFrame(results[\"responses\"], schema=schema)\n",
    "        .select(\"qid\", \"hits.*\")\n",
    "        .withColumn(\"total\", F.col(\"total.value\"))\n",
    "    )\n",
    "\n",
    "    window = Window.partitionBy(\"qid\").orderBy(\"pos\")\n",
    "    return (\n",
    "        resp.select(\n",
    "            \"qid\", \"total\", \"max_score\", F.posexplode(\"hits\").alias(\"pos\", \"hits\")\n",
    "        )\n",
    "        .withColumn(\"docids\", F.collect_list(F.col(\"hits._id\")).over(window))\n",
    "        .withColumn(\"scores\", F.collect_list(F.col(\"hits._score\")).over(window))\n",
    "        .groupBy(\"qid\")\n",
    "        .agg(\n",
    "            F.any_value(\"total\").alias(\"total\"),\n",
    "            F.any_value(\"max_score\").alias(\"max_score\"),\n",
    "            F.max(\"docids\").alias(\"docids\"),\n",
    "            F.max(\"scores\").alias(\"scores\"),\n",
    "        )\n",
    "        .join(\n",
    "            relevant_queries.select(\"qid\", \"rel_docids\"),\n",
    "            on=\"qid\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"computed_scores\",\n",
    "            compute_scores(F.col(\"docids\"), F.col(\"scores\"), F.col(\"rel_docids\")),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:opensearch:POST http://localhost:9200/_msearch [status:200 request:2.077s]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|              qid|total|max_score|              docids|              scores|          rel_docids|     computed_scores|\n",
      "+-----------------+-----+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  q01238589934750|  325| 18.65921|[doc012303114671,...|[18.65921, 18.654...|[doc012311714403,...|{0.34561852186877...|\n",
      "| q012334359739164|10000|26.772526|[doc012302714978,...|[26.772526, 26.71...|[doc012311108501,...|{0.63216048917480...|\n",
      "| q012360129542513|10000|14.505418|[doc012305205192,...|[14.505418, 14.49...|[doc012311313198,...|{0.44020499676612...|\n",
      "| q012360129543646|10000|  12.9868|[doc012312210422,...|[12.9868, 12.9073...|[doc012312210422,...|            {1.0, 1}|\n",
      "|q0123103079215891|10000|20.341152|[doc012301119210,...|[20.341152, 19.73...|[doc012301119210,...|{0.94563527046735...|\n",
      "+-----------------+-----+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|               ndcg|\n",
      "+-------+-------------------+\n",
      "|  count|                597|\n",
      "|   mean|0.44120878362596844|\n",
      "| stddev| 0.2897845483148074|\n",
      "|    min|                0.0|\n",
      "|    max|                1.0|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "res = run_search(relevant_queries, \"longeval-train-english-2023_01\").cache()\n",
    "res.show(n=5)\n",
    "# average ndcg score, how accurate is this?\n",
    "res.select(\"computed_scores.ndcg\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- qid: string (nullable = true)\n",
      " |-- run: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- qrels: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      "\n",
      "+----------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|             qid|                                                                                                 run|                                                                                               qrels|\n",
      "+----------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "| q01238589934750|{doc012301806966 -> 14.238972, doc012311312899 -> 14.43931, doc012305906574 -> 16.984085, doc0123...|{doc012307806408 -> 1, doc012300910100 -> 1, doc012306604958 -> 1, doc012307704340 -> 1, doc01231...|\n",
      "|q012334359739164|{doc012306916356 -> 20.805424, doc012310507020 -> 22.28429, doc012308012158 -> 21.194016, doc0123...|{doc012301118925 -> 1, doc012308601613 -> 1, doc012312006050 -> 1, doc012312210178 -> 1, doc01230...|\n",
      "|q012360129542513|{doc012306600622 -> 14.353978, doc012312006368 -> 14.286597, doc012308911282 -> 14.362784, doc012...|{doc012311313198 -> 1, doc012309211286 -> 1, doc012309508292 -> 1, doc012312601743 -> 1, doc01230...|\n",
      "+----------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-----------------+\n",
      "|summary|                map|               ndcg|              qid|\n",
      "+-------+-------------------+-------------------+-----------------+\n",
      "|  count|                597|                597|              597|\n",
      "|   mean|0.16477464955560864|0.31424901046884035|             NULL|\n",
      "| stddev|0.19069715746288896|0.25924436825908315|             NULL|\n",
      "|    min|                0.0|                0.0|q0123103079215124|\n",
      "|    max| 0.8491452991452991| 0.9491546172962253|          q012396|\n",
      "+-------+-------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytrec_eval\n",
    "\n",
    "\n",
    "@F.udf(\"map<string, float>\")\n",
    "def run_udf(docids: list[str], scores: list[float]) -> dict[str, int]:\n",
    "    if not docids:\n",
    "        return {}\n",
    "    return {k: v for k, v in zip(docids, scores)}\n",
    "\n",
    "\n",
    "@F.udf(\"map<string, int>\")\n",
    "def qrel_udf(rel_docids: list[str]) -> dict[str, int]:\n",
    "    if not rel_docids:\n",
    "        return {}\n",
    "    # return the relevant documents\n",
    "    return {docid: 1 for docid in rel_docids}\n",
    "\n",
    "\n",
    "# for each qid, we need a mapping of docid to relevance\n",
    "run_df = res.select(\n",
    "    \"qid\",\n",
    "    run_udf(\"docids\", \"scores\").alias(\"run\"),\n",
    "    qrel_udf(\"rel_docids\").alias(\"qrels\"),\n",
    ")\n",
    "run_df.printSchema()\n",
    "run_df.show(n=3, truncate=100)\n",
    "\n",
    "# now convert to the format required by trec_eval\n",
    "qrel = {}\n",
    "run = {}\n",
    "for row in run_df.collect():\n",
    "    qrel[row.qid] = row.qrels\n",
    "    run[row.qid] = row.run\n",
    "\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrel, {\"ndcg\", \"map\"})\n",
    "evals = evaluator.evaluate(run)\n",
    "\n",
    "evals_df = spark.createDataFrame([{\"qid\": k, **v} for k, v in evals.items()])\n",
    "evals_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------------+\n",
      "|              qid|           my_ndcg|          trec_ndcg|\n",
      "+-----------------+------------------+-------------------+\n",
      "|  q01238589934750|0.3456185218687763|0.17057078643005402|\n",
      "| q012334359739164|0.6321604891748087|  0.564020444522855|\n",
      "| q012360129542513|0.4402049967661204| 0.2578466240460916|\n",
      "| q012360129543646|               1.0|0.27487633291429087|\n",
      "|q0123103079215891|0.9456352704673574|  0.843706044175124|\n",
      "|q0123111669150911|               0.0|                0.0|\n",
      "|q0123120259085172|0.4753190249891079|0.28593204868013994|\n",
      "|q0123120259086252|0.1790522317510419|  0.054181637470214|\n",
      "| q012317179871244|0.4306765580733929|0.13032376591037062|\n",
      "|        q01231876|0.7996199827438175| 0.7996199827438173|\n",
      "| q012325769805427|               0.0|                0.0|\n",
      "| q012334359739354|0.5221860067660785| 0.3676841955471875|\n",
      "|  q01238589936278|               0.0|                0.0|\n",
      "|q0123103079216681|               0.0|                0.0|\n",
      "| q012325769805594| 0.326343148574143| 0.2529643212795723|\n",
      "| q012342949674351|0.6162190698192223| 0.6175361152358866|\n",
      "| q012394489281275|               0.0|                0.0|\n",
      "| q012317179870904|0.5057165869077156|  0.505716586907716|\n",
      "| q012351539608433|0.5816582868512138| 0.4508714045103576|\n",
      "|q0123111669151117|0.7748634138662909| 0.7239774934883926|\n",
      "+-----------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# okay let's see where we went wrong\n",
    "\n",
    "debug = res.select(\"qid\", F.col(\"computed_scores.ndcg\").alias(\"my_ndcg\")).join(\n",
    "    evals_df.select(\"qid\", \"ndcg\").withColumnRenamed(\"ndcg\", \"trec_ndcg\"),\n",
    "    on=\"qid\",\n",
    ")\n",
    "debug.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so I have no idea why my ndcg stuff doesn't work and overestimates the score. I'm just going to use the tried and true library -- might be worth figuring out how to reproduce the scores between the two systems at some point though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
